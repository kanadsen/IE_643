{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b1310635-1c39-4abc-b49b-b0ae06996e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import evals\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Function for shuffling the dataset\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Module Imports\n",
    "#from model import VAE, compute_loss\n",
    "from utils import build_path, get_label, get_feat, THRESHOLDS\n",
    "\n",
    "# Torch Imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import log_normal, log_normal_mixture\n",
    "# Import the pretrained default model resnet18/resnet50/resnet101\n",
    "from torchvision.models import resnet50,resnet152,resnet101,efficientnet_v2_l\n",
    "\n",
    "device = torch.device( \"cpu\") #\"cuda:0\" if torch.cuda.is_available() else\n",
    "\n",
    "class base_class(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(base_class, self).__init__()\n",
    "        self.args=args\n",
    "        self.model=resnet50() #weights='ResNet101_Weights.IMAGENET1K_V1'\n",
    "        self.model.fc=nn.Flatten() # Flatten the last layer\n",
    "        print(self.model)\n",
    "    def forward(self,data):\n",
    "        return self.model(data)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VAE, self).__init__()\n",
    "        self.args = args\n",
    "        self.dropout = nn.Dropout(p=args.drop)\n",
    "\n",
    "        self.base_model=base_class(args)\n",
    "        \n",
    "        \"\"\"Feature encoder\"\"\"\n",
    "        self.fx = nn.Sequential(\n",
    "            nn.Linear(args.feature_dim, 256), # Set args.feature dim according to flatten shape. By default it is 2048\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(256, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(512, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(512, 256,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout\n",
    "        )\n",
    "        self.fx_mu = nn.Linear(256, args.latent_dim,bias=True)\n",
    "        self.fx_logvar = nn.Linear(256, args.latent_dim,bias=True)\n",
    "\n",
    "        \"\"\"Label encoder\"\"\"\n",
    "        self.label_lookup = nn.Linear(args.label_dim, args.emb_size)\n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Linear(args.emb_size, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(512, 256,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout\n",
    "        )\n",
    "        self.fe_mu = nn.Linear(256, args.latent_dim,bias=True)\n",
    "        self.fe_logvar = nn.Linear(256, args.latent_dim,bias=True)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        self.fd = nn.Sequential(\n",
    "            nn.Linear(args.feature_dim + args.latent_dim, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, args.emb_size,bias=True),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def label_encode(self, x):\n",
    "        h0 = self.dropout(F.relu(self.label_lookup(x)))\n",
    "        h = self.fe(h0)\n",
    "        mu = self.fe_mu(h)\n",
    "        logvar = self.fe_logvar(h)\n",
    "        fe_output = {\n",
    "            'fe_mu': mu,\n",
    "            'fe_logvar': logvar\n",
    "        }\n",
    "        return fe_output\n",
    "\n",
    "    def feat_encode(self, x):\n",
    "        #print(x.shape)\n",
    "        h = self.fx(x)\n",
    "        mu = self.fx_mu(h)\n",
    "        logvar = self.fx_logvar(h)\n",
    "        fx_output = {\n",
    "            'fx_mu': mu,\n",
    "            'fx_logvar': logvar\n",
    "        }\n",
    "        return fx_output\n",
    "\n",
    "    def decode(self, z):\n",
    "        d = self.fd(z)\n",
    "        d = F.normalize(d, dim=1)\n",
    "        return d\n",
    "\n",
    "    def label_forward(self, x, feat):\n",
    "        n_label = x.shape[1]\n",
    "        all_labels = torch.eye(n_label).to(device)\n",
    "        fe_output = self.label_encode(all_labels)\n",
    "        mu = fe_output['fe_mu']\n",
    "        \n",
    "        z = torch.matmul(x, mu) / x.sum(1, keepdim=True)\n",
    "        print(feat.shape,z.shape)\n",
    "        label_emb = self.decode(torch.cat((feat, z), 1))\n",
    "\n",
    "        fe_output['label_emb'] = label_emb\n",
    "        return fe_output\n",
    "\n",
    "    def feat_forward(self, x):\n",
    "        fx_output = self.feat_encode(x)\n",
    "        mu = fx_output['fx_mu']\n",
    "        logvar = fx_output['fx_logvar']\n",
    "\n",
    "        if not self.training:\n",
    "            z = mu\n",
    "            z2 = mu\n",
    "        else:\n",
    "            z = reparameterize(mu, logvar)\n",
    "            z2 = reparameterize(mu, logvar)\n",
    "        feat_emb = self.decode(torch.cat((x, z), 1))\n",
    "        feat_emb2 = self.decode(torch.cat((x, z2), 1))\n",
    "        fx_output['feat_emb'] = feat_emb\n",
    "        fx_output['feat_emb2'] = feat_emb2\n",
    "        return fx_output\n",
    "\n",
    "    def forward(self, label, feature):\n",
    "        # Apply resnet model to get feature embeddings\n",
    "        feature=self.base_model(feature)\n",
    "        fe_output = self.label_forward(label, feature)\n",
    "        label_emb = fe_output['label_emb']\n",
    "        fx_output = self.feat_forward(feature)\n",
    "        feat_emb, feat_emb2 = fx_output['feat_emb'], fx_output['feat_emb2']\n",
    "\n",
    "        embs = self.label_lookup.weight\n",
    "        label_out = torch.matmul(label_emb, embs)\n",
    "        feat_out = torch.matmul(feat_emb, embs)\n",
    "        feat_out2 = torch.matmul(feat_emb2, embs)\n",
    "        \n",
    "        fe_output.update(fx_output)\n",
    "        output = fe_output\n",
    "        output['embs'] = embs\n",
    "        output['label_out'] = label_out\n",
    "        output['feat_out'] = feat_out\n",
    "        output['feat_out2'] = feat_out2\n",
    "        output['feat'] = feature\n",
    "        return output\n",
    "\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps*std\n",
    "\n",
    "\n",
    "def compute_loss(input_label, output, args=None):\n",
    "    fe_out, fe_mu, fe_logvar, label_emb = \\\n",
    "        output['label_out'], output['fe_mu'], output['fe_logvar'], output['label_emb']\n",
    "    fx_out, fx_mu, fx_logvar, feat_emb = \\\n",
    "        output['feat_out'], output['fx_mu'], output['fx_logvar'], output['feat_emb']\n",
    "    fx_out2 = output['feat_out2']\n",
    "    embs = output['embs']\n",
    "\n",
    "    fx_sample = reparameterize(fx_mu, fx_logvar)\n",
    "    fx_var = torch.exp(fx_logvar)\n",
    "    fe_var = torch.exp(fe_logvar)\n",
    "    kl_loss = (log_normal(fx_sample, fx_mu, fx_var) - \\\n",
    "        log_normal_mixture(fx_sample, fe_mu, fe_var, input_label)).mean()\n",
    "\n",
    "    pred_e = torch.sigmoid(fe_out)\n",
    "    pred_x = torch.sigmoid(fx_out)\n",
    "    pred_x2 = torch.sigmoid(fx_out2)\n",
    "\n",
    "    def compute_BCE_and_RL_loss(E):\n",
    "        #compute negative log likelihood (BCE loss) for each sample point\n",
    "        sample_nll = -(\n",
    "            torch.log(E) * input_label + torch.log(1 - E) * (1 - input_label)\n",
    "        )\n",
    "        logprob = -torch.sum(sample_nll, dim=2)\n",
    "\n",
    "        #the following computation is designed to avoid the float overflow (log_sum_exp trick)\n",
    "        maxlogprob = torch.max(logprob, dim=0)[0]\n",
    "        Eprob = torch.mean(torch.exp(logprob - maxlogprob), axis=0)\n",
    "        nll_loss = torch.mean(-torch.log(Eprob) - maxlogprob)\n",
    "        return nll_loss\n",
    "\n",
    "    def supconloss(label_emb, feat_emb, embs, temp=1.0):\n",
    "        features = torch.cat((label_emb, feat_emb))\n",
    "        labels = torch.cat((input_label, input_label)).float()\n",
    "        n_label = labels.shape[1]\n",
    "        emb_labels = torch.eye(n_label).to(device)\n",
    "        mask = torch.matmul(labels, emb_labels)\n",
    "\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, embs),\n",
    "            temp)\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        exp_logits = torch.exp(logits)\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    nll_loss = compute_BCE_and_RL_loss(pred_e.unsqueeze(0))\n",
    "    nll_loss_x = compute_BCE_and_RL_loss(pred_x.unsqueeze(0))\n",
    "    nll_loss_x2 = compute_BCE_and_RL_loss(pred_x2.unsqueeze(0))\n",
    "    sum_nll_loss = nll_loss + nll_loss_x + nll_loss_x2\n",
    "    cpc_loss = supconloss(label_emb, feat_emb, embs)\n",
    "    total_loss = sum_nll_loss * args.nll_coeff + kl_loss * 6. + cpc_loss\n",
    "    return total_loss, nll_loss, nll_loss_x, 0., 0., kl_loss, cpc_loss, pred_e, pred_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./')\n",
    "\n",
    "METRICS = ['ACC', 'HA', 'ebF1', 'miF1', 'maF1', 'meanAUC', 'medianAUC', 'meanAUPR', 'medianAUPR', 'meanFDR', 'medianFDR', 'p_at_1', 'p_at_3', 'p_at_5']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser values definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Create an ArgumentParser object\n",
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "\n",
    "# Device \n",
    "args.device = torch.device(\"cpu\") #\"cuda:0\" if torch.cuda.is_available() else \n",
    "print(args.device)\n",
    "\n",
    "# Dataset name\n",
    "args.dataset='MS_COCO'\n",
    "\n",
    "# General Model variables\n",
    "args.seed=100\n",
    "args.lr=1e-3\n",
    "args.lr_decay_ratio=0.9\n",
    "args.lr_decay_times=4\n",
    "args.nll_coeff=0.5\n",
    "args.l2_coeff=1\n",
    "args.c_coeff=0\n",
    "args.class_weights=1  # Find the class weights depending on count\n",
    "args.current_step=0 # For Plotting the loss and other variables w.r.t iterations\n",
    "\n",
    "# Set Dataloader parameters\n",
    "args.BATCH_SIZE = 64\n",
    "args.NUM_WORKERS = 2\n",
    "args.max_epoch=200\n",
    "\n",
    "# scheduler variables\n",
    "args.eta_min=2e-4\n",
    "args.T_mult=2\n",
    "args.T0=50\n",
    "args.retrain=input(\"Enter True or False if you want to retrain\")=='True'\n",
    "\n",
    "# VAE model variables\n",
    "args.latent_dim=100\n",
    "args.drop=0.1 # dropout value\n",
    "args.feature_dim=2048 # Output dimension from flattened ResNet\n",
    "args.emb_size=100 # What is emb_size ?\n",
    "args.label_dim=78 # Label Dimension\n",
    "args.param_setting = \"lr-{}_lr-decay_{:.2f}_lr-times_{:.1f}_nll-{:.2f}_l2-{:.2f}_c-{:.2f}\".format(args.lr, args.lr_decay_ratio, args.lr_decay_times, args.nll_coeff, args.l2_coeff, args.c_coeff)\n",
    "\n",
    "# Directory variables\n",
    "args.summary_dir = 'summary/{}/{}'.format(args.dataset, args.param_setting)\n",
    "args.model_dir = 'model/model_{}/{}'.format(args.dataset, args.param_setting)\n",
    "args.checkpoint_path=''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Seed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a271060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2800678feb0>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(args.seed) # set the random seed of numpy\n",
    "torch.manual_seed(args.seed) # set same seed for torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "ac1948d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = Path(\"../Dataset/MS_COCO_2017\").absolute() # --- Enter Dataset Path Here ---\n",
    "DATA_DIR = DIR / 'labels'\n",
    "IMG_DIR_TRAIN = DIR / 'imgs/train/'\n",
    "IMG_DIR_TEST = DIR / 'imgs/test/'\n",
    "label_path = DATA_DIR / \"labels/categories.csv\"\n",
    "data_csv= DATA_DIR / 'labels/labels_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9ef10109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hair drier         159.0\n",
      "toaster            181.0\n",
      "parking meter      583.0\n",
      "bear               791.0\n",
      "scissors           792.0\n",
      "                  ...   \n",
      "cup               7662.0\n",
      "dining table      9842.0\n",
      "car              10283.0\n",
      "chair            10612.0\n",
      "person           53529.0\n",
      "Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cats = pd.read_csv(label_path, header=None)\n",
    "cats = list(cats[0])\n",
    "data = pd.read_csv(data_csv, names=[\"Image Name\"] + cats)\n",
    "column_sums_sorted = data.drop(\"Image Name\", axis = 1).sum().sort_values(ascending=True)\n",
    "print(column_sums_sorted)\n",
    "#data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "769122b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97774, 97762)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop([\"hair drier\", \"toaster\"], axis = 1)\n",
    "\n",
    "# Check if all entries except column 0 are 0\n",
    "mask = (df.iloc[:, 1:] != 0).any(axis=1)\n",
    "\n",
    "# Keep rows where at least one entry is not 0\n",
    "filtered_df = df[mask]\n",
    "\n",
    "len(data), len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "320b20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, csv_path, label_path, transformations, type):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            transformations: pytorch transforms for transforms and tensor conversion\n",
    "            train: flag to determine if train or val set\n",
    "        \"\"\"\n",
    "        cats = pd.read_csv(label_path, header=None)\n",
    "        cats = list(cats[0])\n",
    "        data = pd.read_csv(csv_path, names=[\"Image Name\"] + cats)\n",
    "        df = data.drop([\"hair drier\", \"toaster\"], axis = 1)\n",
    "\n",
    "        # Check if all entries except column 0 are 0\n",
    "        mask = (df.iloc[:, 1:] != 0).any(axis=1)\n",
    "\n",
    "        # Keep rows where at least one entry is not 0\n",
    "        filtered_df = df[mask]\n",
    "        \n",
    "        # IMPORTANT: dataset needs to be shuffled because by default,\n",
    "        # the loaded dataset has an ordering which may cause examples of \n",
    "        # certain classes from being missed in the training set if following \n",
    "        # the below method to create a train/val/test split\n",
    "        self.data_info = shuffle(filtered_df)\n",
    "        self.data_info=self.data_info.reset_index()\n",
    "\n",
    "        # Transforms\n",
    "        self.transforms = transformations\n",
    "\n",
    "        # Read the csv file\n",
    "        self.data_feature = list(filtered_df.iloc[:, 0])\n",
    "        self.data_label = np.asarray(filtered_df.iloc[:, 1:])\n",
    "        \n",
    "        # 70 : 15 : 15 split\n",
    "        train_split = int(0.7*len(self.data_info))\n",
    "        val_split = int(0.15*len(self.data_info))\n",
    "\n",
    "        # validation set\n",
    "        if type == \"train\":\n",
    "            self.image_arr = (self.data_feature[:train_split])\n",
    "            self.label_arr = (self.data_label[:train_split])\n",
    "        if type == \"val\":\n",
    "            self.image_arr = (self.data_feature[train_split:train_split+val_split])\n",
    "            self.label_arr = (self.data_label[train_split:train_split+val_split])\n",
    "        if type == \"test\":\n",
    "            self.image_arr = (self.data_feature[train_split+val_split:])\n",
    "            print(len(self.image_arr))\n",
    "            self.label_arr = (self.data_label[train_split+val_split:])\n",
    "\n",
    "        #else:\n",
    "           # raise AttributeError(\"'type' attribute can only be train, val, test\")\n",
    "  \n",
    "        self.label_arr = torch.from_numpy(np.asarray(self.label_arr))\n",
    "        self.data_len = len(self.label_arr)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_name = self.image_arr[index]\n",
    "        \n",
    "        # Open image\n",
    "        img_as_img = Image.open(str(IMG_DIR_TRAIN) + \"/\" + single_image_name).convert('RGB')\n",
    "\n",
    "        img_as_tensor = self.transforms(img_as_img)\n",
    "        \n",
    "        single_image_label = self.label_arr[index]\n",
    "\n",
    "        return  {'images':img_as_tensor.to(torch.float), 'label':single_image_label.to(torch.float)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ccb0038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), # Resize image to 256 x 256 to ensure consistency in input\n",
    "    transforms.RandomHorizontalFlip(), # Data Augmentation\n",
    "    transforms.ToTensor()\n",
    "    # Best values found for normalization in ImageNet (from pytorch documentation)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "7f545b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14665\n"
     ]
    }
   ],
   "source": [
    "train_data = CreateDataset(data_csv, label_path,transform, type=\"train\")\n",
    "val_data = CreateDataset(data_csv, label_path, transform, type=\"val\")\n",
    "test_data = CreateDataset(data_csv, label_path, transform, type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': tensor([[[0.0078, 0.0078, 0.0078,  ..., 0.1882, 0.1569, 0.0667],\n",
       "          [0.0667, 0.0471, 0.0353,  ..., 0.1882, 0.1686, 0.0784],\n",
       "          [0.1137, 0.0902, 0.0902,  ..., 0.2275, 0.1922, 0.0980],\n",
       "          ...,\n",
       "          [0.3490, 0.7373, 0.7804,  ..., 0.6980, 0.7020, 0.7098],\n",
       "          [0.3373, 0.6980, 0.7059,  ..., 0.4078, 0.4275, 0.4000],\n",
       "          [0.3608, 0.7451, 0.7137,  ..., 0.0353, 0.0314, 0.0314]],\n",
       " \n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.1765, 0.1490, 0.0627],\n",
       "          [0.0588, 0.0392, 0.0275,  ..., 0.1725, 0.1529, 0.0667],\n",
       "          [0.1059, 0.0824, 0.0824,  ..., 0.2000, 0.1647, 0.0784],\n",
       "          ...,\n",
       "          [0.2824, 0.6275, 0.6510,  ..., 0.6078, 0.6078, 0.6157],\n",
       "          [0.2745, 0.5882, 0.5765,  ..., 0.3412, 0.3608, 0.3333],\n",
       "          [0.2980, 0.6353, 0.5843,  ..., 0.0039, 0.0039, 0.0039]],\n",
       " \n",
       "         [[0.0196, 0.0196, 0.0118,  ..., 0.1569, 0.1294, 0.0510],\n",
       "          [0.0784, 0.0588, 0.0392,  ..., 0.1490, 0.1333, 0.0549],\n",
       "          [0.1255, 0.0980, 0.0941,  ..., 0.1647, 0.1373, 0.0549],\n",
       "          ...,\n",
       "          [0.2078, 0.4706, 0.4627,  ..., 0.3804, 0.3843, 0.3922],\n",
       "          [0.1961, 0.4235, 0.3725,  ..., 0.2471, 0.2667, 0.2392],\n",
       "          [0.2196, 0.4588, 0.3569,  ..., 0.0078, 0.0000, 0.0000]]]),\n",
       " 'label': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing dataloader\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "9ab0807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data, './data/MS_COCO_2017/train_data.pt')\n",
    "torch.save(val_data, './data/MS_COCO_2017/val_data.pt')\n",
    "torch.save(test_data, './data/MS_COCO_2017/test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "570f8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataloaders for train, val and test data\n",
    "train_loader = DataLoader(train_data, batch_size=args.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=len(val_data))\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "33258e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2835ab9ac40>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x282dadd0d30>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2835ab9ac10>)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "50411bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': tensor([[[[0.1059, 0.1020, 0.0980,  ..., 0.5608, 0.5529, 0.5451],\n",
       "           [0.1059, 0.1020, 0.0980,  ..., 0.5686, 0.5647, 0.5569],\n",
       "           [0.1059, 0.1020, 0.0980,  ..., 0.5608, 0.5608, 0.5569],\n",
       "           ...,\n",
       "           [0.1529, 0.1490, 0.1569,  ..., 0.2000, 0.1882, 0.1176],\n",
       "           [0.1647, 0.1569, 0.1686,  ..., 0.2275, 0.2235, 0.1569],\n",
       "           [0.1686, 0.1569, 0.1686,  ..., 0.2392, 0.2275, 0.1765]],\n",
       " \n",
       "          [[0.0510, 0.0471, 0.0431,  ..., 0.4863, 0.4784, 0.4706],\n",
       "           [0.0510, 0.0471, 0.0431,  ..., 0.4941, 0.4902, 0.4824],\n",
       "           [0.0510, 0.0471, 0.0431,  ..., 0.4863, 0.4863, 0.4824],\n",
       "           ...,\n",
       "           [0.0471, 0.0392, 0.0392,  ..., 0.1529, 0.1451, 0.0784],\n",
       "           [0.0588, 0.0471, 0.0471,  ..., 0.1725, 0.1686, 0.1059],\n",
       "           [0.0627, 0.0471, 0.0510,  ..., 0.1725, 0.1647, 0.1098]],\n",
       " \n",
       "          [[0.1176, 0.1137, 0.1098,  ..., 0.3647, 0.3529, 0.3412],\n",
       "           [0.1176, 0.1137, 0.1098,  ..., 0.3725, 0.3647, 0.3529],\n",
       "           [0.1176, 0.1137, 0.1098,  ..., 0.3647, 0.3608, 0.3529],\n",
       "           ...,\n",
       "           [0.1216, 0.1098, 0.1098,  ..., 0.1804, 0.1765, 0.1137],\n",
       "           [0.1333, 0.1176, 0.1176,  ..., 0.1804, 0.1882, 0.1294],\n",
       "           [0.1373, 0.1216, 0.1216,  ..., 0.1765, 0.1804, 0.1333]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0039,  ..., 0.0431, 0.0078, 0.0039],\n",
       "           [0.0000, 0.0000, 0.0078,  ..., 0.0353, 0.0118, 0.0078],\n",
       "           [0.0000, 0.0000, 0.0118,  ..., 0.0235, 0.0078, 0.0078],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0549,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0196,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0157,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0039,  ..., 0.0353, 0.0078, 0.0078],\n",
       "           [0.0000, 0.0000, 0.0039,  ..., 0.0314, 0.0078, 0.0078],\n",
       "           [0.0039, 0.0000, 0.0000,  ..., 0.0235, 0.0078, 0.0078],\n",
       "           ...,\n",
       "           [0.0000, 0.0039, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0039, 0.0431,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0039, 0.0392,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0039,  ..., 0.0588, 0.0157, 0.0039],\n",
       "           [0.0000, 0.0000, 0.0078,  ..., 0.0549, 0.0196, 0.0078],\n",
       "           [0.0000, 0.0000, 0.0039,  ..., 0.0431, 0.0157, 0.0118],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0588,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0157,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0118,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.1216, 0.1216, 0.1216,  ..., 0.1216, 0.1216, 0.1216],\n",
       "           [0.1216, 0.1216, 0.1216,  ..., 0.1216, 0.1216, 0.1216],\n",
       "           [0.1216, 0.1216, 0.1216,  ..., 0.1216, 0.1216, 0.1216],\n",
       "           ...,\n",
       "           [0.1216, 0.1216, 0.1216,  ..., 0.1216, 0.1216, 0.1216],\n",
       "           [0.1216, 0.1216, 0.1216,  ..., 0.1216, 0.1216, 0.1216],\n",
       "           [0.1216, 0.1216, 0.1216,  ..., 0.1216, 0.1216, 0.1216]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "           [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "           [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "           ...,\n",
       "           [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "           [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "           [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.8078, 0.8078, 0.8039,  ..., 0.7843, 0.7529, 0.7412],\n",
       "           [0.8118, 0.8118, 0.8078,  ..., 0.7922, 0.7608, 0.7490],\n",
       "           [0.8157, 0.8118, 0.8118,  ..., 0.8118, 0.7765, 0.7569],\n",
       "           ...,\n",
       "           [0.0824, 0.0784, 0.0824,  ..., 0.4353, 0.4275, 0.4353],\n",
       "           [0.0784, 0.0745, 0.0784,  ..., 0.4431, 0.4392, 0.4353],\n",
       "           [0.0784, 0.0784, 0.0745,  ..., 0.4353, 0.4392, 0.4353]],\n",
       " \n",
       "          [[0.8078, 0.8078, 0.8039,  ..., 0.8157, 0.8000, 0.8000],\n",
       "           [0.8118, 0.8118, 0.8078,  ..., 0.8235, 0.8039, 0.8000],\n",
       "           [0.8157, 0.8118, 0.8118,  ..., 0.8353, 0.8157, 0.8039],\n",
       "           ...,\n",
       "           [0.0824, 0.0784, 0.0824,  ..., 0.2078, 0.2000, 0.2078],\n",
       "           [0.0784, 0.0745, 0.0784,  ..., 0.2157, 0.2118, 0.2078],\n",
       "           [0.0784, 0.0784, 0.0745,  ..., 0.2078, 0.2118, 0.2078]],\n",
       " \n",
       "          [[0.8078, 0.8078, 0.8039,  ..., 0.8588, 0.8549, 0.8627],\n",
       "           [0.8118, 0.8118, 0.8078,  ..., 0.8667, 0.8588, 0.8667],\n",
       "           [0.8157, 0.8118, 0.8118,  ..., 0.8745, 0.8667, 0.8667],\n",
       "           ...,\n",
       "           [0.0902, 0.0863, 0.0902,  ..., 0.2000, 0.1922, 0.2000],\n",
       "           [0.0863, 0.0824, 0.0863,  ..., 0.2078, 0.2039, 0.2000],\n",
       "           [0.0863, 0.0863, 0.0824,  ..., 0.1961, 0.2000, 0.1961]]],\n",
       " \n",
       " \n",
       "         [[[0.4235, 0.3098, 0.3765,  ..., 0.2745, 0.3373, 0.2980],\n",
       "           [0.3373, 0.2980, 0.4275,  ..., 0.2824, 0.3176, 0.2902],\n",
       "           [0.1843, 0.2157, 0.3412,  ..., 0.3098, 0.3216, 0.2667],\n",
       "           ...,\n",
       "           [0.6627, 0.6549, 0.6510,  ..., 0.6745, 0.6980, 0.6745],\n",
       "           [0.6431, 0.6196, 0.5961,  ..., 0.6863, 0.7059, 0.6941],\n",
       "           [0.6039, 0.5765, 0.5725,  ..., 0.7137, 0.7529, 0.7176]],\n",
       " \n",
       "          [[0.4275, 0.3216, 0.3961,  ..., 0.2196, 0.3020, 0.2824],\n",
       "           [0.3490, 0.3137, 0.4510,  ..., 0.2588, 0.3020, 0.2824],\n",
       "           [0.2039, 0.2431, 0.3725,  ..., 0.2706, 0.2745, 0.2196],\n",
       "           ...,\n",
       "           [0.6667, 0.6588, 0.6549,  ..., 0.6745, 0.6980, 0.6745],\n",
       "           [0.6471, 0.6235, 0.6000,  ..., 0.6863, 0.7059, 0.6941],\n",
       "           [0.6078, 0.5804, 0.5765,  ..., 0.7137, 0.7529, 0.7176]],\n",
       " \n",
       "          [[0.4078, 0.3137, 0.4039,  ..., 0.2157, 0.2902, 0.2667],\n",
       "           [0.3294, 0.3137, 0.4627,  ..., 0.2000, 0.2314, 0.2078],\n",
       "           [0.1961, 0.2471, 0.3961,  ..., 0.2039, 0.1882, 0.1255],\n",
       "           ...,\n",
       "           [0.6745, 0.6667, 0.6627,  ..., 0.6745, 0.6980, 0.6745],\n",
       "           [0.6549, 0.6314, 0.6078,  ..., 0.6863, 0.7059, 0.6941],\n",
       "           [0.6157, 0.5882, 0.5843,  ..., 0.7137, 0.7529, 0.7176]]]]),\n",
       " 'label': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function call definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "6ca2d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#building the model \n",
    "model = VAE(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (base_model): base_class(\n",
       "    (model): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (fx): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "    (9): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (fx_mu): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (fx_logvar): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (label_lookup): Linear(in_features=78, out_features=100, bias=True)\n",
       "  (fe): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (fe_mu): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (fe_logvar): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (fd): Sequential(\n",
       "    (0): Linear(in_features=2148, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log the learning rate \n",
    "writer = SummaryWriter(log_dir=args.summary_dir)\n",
    "writer.add_scalar('learning_rate', args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Define Optimizer\n",
    "args.optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=5e-4)\n",
    "args.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(args.optimizer, eta_min=args.eta_min, T_0=args.T0, T_mult=args.T_mult)\n",
    "print(args.retrain)\n",
    "if args.retrain==True:\n",
    "    model.load_state_dict(torch.load(args.checkpoint_path))\n",
    "    print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the losses\n",
    "\n",
    "# smooth means average. Every batch has a mean loss value w.r.t. different losses\n",
    "smooth_nll_loss=0.0 # label encoder decoder cross entropy loss\n",
    "smooth_nll_loss_x=0.0 # feature encoder decoder cross entropy loss\n",
    "smooth_c_loss = 0.0 # label encoder decoder ranking loss\n",
    "smooth_c_loss_x=0.0 # feature encoder decoder ranking loss\n",
    "smooth_kl_loss = 0.0 # kl divergence\n",
    "smooth_total_loss=0.0 # total loss\n",
    "smooth_macro_f1 = 0.0 # macro_f1 score\n",
    "smooth_micro_f1 = 0.0 # micro_f1 score\n",
    "\n",
    "best_loss = 1e10\n",
    "best_iter = 0\n",
    "best_macro_f1 = 0.0 # best macro f1 for ckpt selection in validation\n",
    "best_micro_f1 = 0.0 # best micro f1 for ckpt selection in validation\n",
    "best_acc = 0.0 # best subset acc for ckpt selction in validation\n",
    "\n",
    "temp_label=[]\n",
    "temp_pred_x=[]\n",
    "\n",
    "\n",
    "best_test_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Function\n",
    "def train(model,train_loader, args):\n",
    "    print(\"Training Started\")\n",
    "    counter=0\n",
    "    model=model.to(args.device)\n",
    "    # smooth means average. Every batch has a mean loss value w.r.t. different losses\n",
    "    smooth_nll_loss=0.0 # label encoder decoder cross entropy loss\n",
    "    smooth_nll_loss_x=0.0 # feature encoder decoder cross entropy loss\n",
    "    smooth_c_loss = 0.0 # label encoder decoder ranking loss\n",
    "    smooth_c_loss_x=0.0 # feature encoder decoder ranking loss\n",
    "    smooth_kl_loss = 0.0 # kl divergence\n",
    "    smooth_total_loss=0.0 # total loss\n",
    "    smooth_macro_f1 = 0.0 # macro_f1 score\n",
    "    smooth_micro_f1 = 0.0 # micro_f1 score\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        print(\"Entered Loop\")\n",
    "        x = data['images'].to(args.device)\n",
    "        print(\"Got x\")\n",
    "        targets = data['label'].to(args.device)\n",
    "        args.optimizer.zero_grad()\n",
    "        output = model(targets,x)\n",
    "        print(\"Output Done!\")\n",
    "        total_loss, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, cpc_loss, _, pred_x = \\\n",
    "                    compute_loss(targets, output, args)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        args.optimizer.step()\n",
    "\n",
    "        train_metrics = evals.compute_metrics(pred_x.cpu().data.numpy(), targets.cpu().data.numpy(), 0.5, all_metrics=False)\n",
    "        macro_f1, micro_f1 = train_metrics['maF1'], train_metrics['miF1']\n",
    "       \n",
    "        smooth_nll_loss += nll_loss\n",
    "        smooth_nll_loss_x += nll_loss_x\n",
    "        smooth_c_loss += c_loss\n",
    "        smooth_c_loss_x += c_loss_x\n",
    "        smooth_kl_loss += kl_loss\n",
    "        smooth_total_loss += total_loss\n",
    "        smooth_macro_f1 += macro_f1\n",
    "        smooth_micro_f1 += micro_f1\n",
    "\n",
    "        counter+=1\n",
    "        print(\"Train Func\",counter)\n",
    "        del x,targets,outputs,\n",
    "        \n",
    "    nll_loss = smooth_nll_loss / counter\n",
    "    nll_loss_x = smooth_nll_loss_x / counter\n",
    "    c_loss = smooth_c_loss / counter\n",
    "    c_loss_x = smooth_c_loss_x / counter\n",
    "    kl_loss = smooth_kl_loss / counter\n",
    "    total_loss = smooth_total_loss / counter\n",
    "    macro_f1 = smooth_macro_f1 / counter\n",
    "    micro_f1 = smooth_micro_f1 / counter\n",
    "       \n",
    "   \n",
    "    return model, train_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "test_temp_label = []\n",
    "test_temp_pred_x = []\n",
    "def test(model, device, test_loader):\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "     for i, data in enumerate(test_loader):\n",
    "        x_test = data['images'].to(device)\n",
    "        targets = data['label'].to(device)\n",
    "        output = model(targets,x_test)\n",
    "\n",
    "        total_loss, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, cpc_loss, _, pred_x = \\\n",
    "                    compute_loss(targets, output, args)\n",
    "\n",
    "        test_metrics = evals.compute_metrics(pred_x.cpu().data.numpy(), targets.cpu().data.numpy(), 0.5, all_metrics=False)\n",
    "        macro_f1, micro_f1 = test_metrics['maF1'], test_metrics['miF1']\n",
    "       \n",
    "        smooth_nll_loss += nll_loss\n",
    "        smooth_nll_loss_x += nll_loss_x\n",
    "        smooth_c_loss += c_loss\n",
    "        smooth_c_loss_x += c_loss_x\n",
    "        smooth_kl_loss += kl_loss\n",
    "        smooth_total_loss += total_loss\n",
    "        smooth_macro_f1 += macro_f1\n",
    "        smooth_micro_f1 += micro_f1\n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "    nll_loss = smooth_nll_loss / counter\n",
    "    nll_loss_x = smooth_nll_loss_x / counter\n",
    "    c_loss = smooth_c_loss / counter\n",
    "    c_loss_x = smooth_c_loss_x / counter\n",
    "    kl_loss = smooth_kl_loss / counter\n",
    "    total_loss = smooth_total_loss / counter\n",
    "    macro_f1 = smooth_macro_f1 / counter\n",
    "    micro_f1 = smooth_micro_f1 / counter\n",
    "          \n",
    "    return test_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, train_loader, device, args):\n",
    "\n",
    "    for epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "        model, train_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1= train(model, train_loader, args)\n",
    "        train_acc=train_metrics['ACC']\n",
    "        train_ha_acc=train_metrics['HA']\n",
    "        print('- Epoch :', epoch+1)\n",
    "        print('*** Training Metrics ***')\n",
    "        print('- NLL Loss : %.5f' % nll_loss,'- Total Loss : %.5f' % total_loss, '- Total Accuracy : %.3f',train_acc,'- Hamming Accuracy : %.3f',train_ha_acc)\n",
    "       \n",
    "        # Validation\n",
    "        test_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1=test(model,val_loader,args)\n",
    "        test_acc=test_metrics['ACC']\n",
    "        test_ha_acc=test_metrics['HA']\n",
    "        print('- Epoch :', epoch+1)\n",
    "        print('*** Training Metrics ***')\n",
    "        print('- NLL Loss : %.5f' % nll_loss,'- Total Loss : %.5f' % total_loss, '- Total Accuracy : %.3f',test_acc,'- Hamming Accuracy : %.3f',test_ha_acc)\n",
    "       \n",
    "        '''\n",
    "        # Learning rate Updatation scheme when loss not decreasing\n",
    "        if(epoch!=0):\n",
    "            #print(train_loss[int(epoch)],train_loss[int(epoch-1)])\n",
    "            if(round(list_train_loss[epoch],5)>=round(list_train_loss[epoch-1],5)):\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "        if(train_loss<0.0002 and counter>=10):\n",
    "           counter=0\n",
    "           args.lr = optimizer.param_groups[0]['lr']*0.5\n",
    "           print(\"\\nLearning rate changed :\",args.lr)\n",
    "           optimizer = optim.Adam(model.parameters(), lr=args.lr,weight_decay=5e-4)\n",
    "           scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "\n",
    "\n",
    "        val_score,_,_,_=test(model, device,test_loader, args)\n",
    "   \n",
    "        # Saving the best model which has the highest R2 value\n",
    "        if(val_score>val_r2_score):  # and score>train_r2_score  # Changed\n",
    "            val_r2_score=val_score\n",
    "            train_r2_score=score   \n",
    "            print(\"Saving Model of score:\",val_score,score)       \n",
    "            save_checkpoint(epoch, model, optimizer, 'ECC_model.pt')  # Saving the best model\n",
    "        '''   \n",
    "        args.scheduler.step()\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "89a75857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Loop\n",
      "Got x\n",
      "torch.Size([64, 2048]) torch.Size([64, 100])\n",
      "Output Done!\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'smooth_nll_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\IE_643\\Project_IE643\\c-gmvae-master\\Final_Run.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Call the train model here\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m args \u001b[39m=\u001b[39m experiment(model, train_loader, device, args)\n",
      "\u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\IE_643\\Project_IE643\\c-gmvae-master\\Final_Run.ipynb Cell 32\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mmax_epoch):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model, train_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1\u001b[39m=\u001b[39m train(model, train_loader, args)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_acc\u001b[39m=\u001b[39mtrain_metrics[\u001b[39m'\u001b[39m\u001b[39mACC\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_ha_acc\u001b[39m=\u001b[39mtrain_metrics[\u001b[39m'\u001b[39m\u001b[39mHA\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\IE_643\\Project_IE643\\c-gmvae-master\\Final_Run.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train_metrics \u001b[39m=\u001b[39m evals\u001b[39m.\u001b[39mcompute_metrics(pred_x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy(), targets\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy(), \u001b[39m0.5\u001b[39m, all_metrics\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m macro_f1, micro_f1 \u001b[39m=\u001b[39m train_metrics[\u001b[39m'\u001b[39m\u001b[39mmaF1\u001b[39m\u001b[39m'\u001b[39m], train_metrics[\u001b[39m'\u001b[39m\u001b[39mmiF1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m smooth_nll_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nll_loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m smooth_nll_loss_x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nll_loss_x\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kanad/Desktop/Github%20repos/IE_643/Project_IE643/c-gmvae-master/Final_Run.ipynb#X44sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m smooth_c_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m c_loss\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'smooth_nll_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Call the train model here\n",
    "\n",
    "args = experiment(model, train_loader, device, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
